# -*- coding: utf-8 -*-
"""botsmiths-chatbot-backend.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b8oPnbw3B2AYep3D4IfVFw1_QKfNbIXh
"""

# !pip install flask==2.0.1 flask-cors ultralytics transformers torch torchvision Werkzeug==2.3.7 waitress pyngrok

######################33

from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
from PIL import Image
import base64
import io
import gc
import warnings
import logging
from typing import List, Dict, Optional
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoProcessor, AutoModelForCausalLM
from ultralytics import YOLO
from pyngrok import ngrok
import os

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configure warnings and GPU memory
torch.cuda.empty_cache()
warnings.filterwarnings("ignore")

def get_device():
    return "cuda" if torch.cuda.is_available() else "cpu"

device = get_device()
app = Flask(__name__)

# Configure CORS to accept requests from any origin
CORS(app, resources={r"/*": {
    "origins": [
            "https://botsmiths-chatbot.vercel.app",
            "http://localhost:3000"
        ],
    "methods": ["GET", "POST", "OPTIONS"],
    "allow_headers": ["Content-Type", "Authorization"]
}})

class ImageChatbot:
    def __init__(self, model_paths: Dict[str, str]):
        self.model_paths = model_paths
        self.yolo_model: Optional[YOLO] = None
        self.git_model: Optional[AutoModelForCausalLM] = None
        self.git_processor: Optional[AutoProcessor] = None
        self.llm_model: Optional[AutoModelForSeq2SeqLM] = None
        self.llm_tokenizer: Optional[AutoTokenizer] = None
        self.conversation_history: List[str] = []
        self.current_image: Optional[str] = None
        self.current_objects: Optional[List[List[float]]] = None
        self.current_caption: Optional[str] = None
        self.models_loaded = False

    def load_models(self):
        if not self.models_loaded:
            try:
                logger.info("Loading YOLO model...")
                self.yolo_model = YOLO(self.model_paths['yolo'])

                logger.info("Loading GIT model...")
                self.git_processor = AutoProcessor.from_pretrained(self.model_paths['git_processor'])
                self.git_model = AutoModelForCausalLM.from_pretrained(self.model_paths['git_model'])

                logger.info("Loading LLM model...")
                self.llm_tokenizer = AutoTokenizer.from_pretrained(self.model_paths['llm_tokenizer'])
                self.llm_model = AutoModelForSeq2SeqLM.from_pretrained(self.model_paths['llm_model'])

                self.models_loaded = True
                logger.info("All models loaded successfully.")
                return "Models loaded successfully"
            except Exception as e:
                logger.error(f"Error loading models: {e}")
                return f"Error loading models: {e}"
        else:
            return "Models already loaded"

    def preprocess_image(self, image: Image.Image) -> Image.Image:
        return image#.resize((512, 512), Image.Resampling.LANCZOS)

    def detect_objects(self, image: Image.Image) -> Optional[List[List[float]]]:
        results = self.yolo_model(image)
        self.current_objects = results[0].boxes.data.tolist()
        return self.current_objects if self.current_objects else None

    def generate_caption(self, image: Image.Image) -> str:
        inputs = self.git_processor(images=image, return_tensors="pt")
        with torch.no_grad():
            generated_ids = self.git_model.generate(pixel_values=inputs.pixel_values, max_length=50)
        self.current_caption = self.git_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return self.current_caption

    def generate_response(self, user_input: str) -> str:
        context = "\n".join(self.conversation_history[-8:])  # Keep only last 8 exchanges
        if self.current_caption:
            context += f"\nIMPORTANT - Image caption: {self.current_caption}"
        if self.current_objects:
            object_names = [str(obj[6]) for obj in self.current_objects if len(obj) > 6]
            if object_names:
                context += f"\nAdditional information - Some objects in the image: {', '.join(object_names[:5])}"
        context += "\nPlease focus primarily on the image caption when responding to the user's question."

        prompt = f"You are an AI assistant that helps analyze images. Here's the context:\n\n{context}\n\nHuman: {user_input}\nAI:"

        input_ids = self.llm_tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=512).to(self.llm_model.device)
        with torch.no_grad():
            output = self.llm_model.generate(input_ids, max_length=250, num_return_sequences=1)
        response = self.llm_tokenizer.decode(output[0], skip_special_tokens=True)

        self.conversation_history.append(f"Human: {user_input}")
        self.conversation_history.append(f"AI: {response}")
        if len(self.conversation_history) > 16:  # Keep only last 8 exchanges
            self.conversation_history = self.conversation_history[-16:]

        return response

    def process_image(self, image: Image.Image) -> str:
        try:
            image = self.preprocess_image(image)
            self.detect_objects(image)
            self.generate_caption(image)
            return f"Image processed. Caption: {self.current_caption}"
        except Exception as e:
            logger.error(f"Error processing image: {e}")
            return f"Error processing image: {e}"

    def chat(self, user_input: str, image: Optional[Image.Image] = None) -> str:
        if not all([self.yolo_model, self.git_model, self.llm_model]):
            return "Models are not loaded. Please call load_models() first."

        if image:
            return self.process_image(image)
        else:
            return self.generate_response(user_input)

    def cleanup(self):
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        for attr in ['yolo_model', 'git_model', 'git_processor', 'llm_model', 'llm_tokenizer']:
            if hasattr(self, attr):
                setattr(self, attr, None)

        self.conversation_history.clear()
        self.current_image = None
        self.current_objects = None
        self.current_caption = None

        gc.collect()

# Initialize the chatbot with model paths
model_paths = {
    'yolo': ".\models\yolov10m.pt",
    'git_processor': "microsoft/git-large",
    'git_model': "microsoft/git-large",
    'llm_tokenizer': "google/flan-t5-base",
    'llm_model': "google/flan-t5-base"
}
chatbot = ImageChatbot(model_paths)

# Add a new route to get the server status and URL
@app.route('/status', methods=['GET'])
def get_status():
    return jsonify({
        "status": "online",
        "server_url": ngrok.get_tunnels()[0].public_url if ngrok.get_tunnels() else None
    })

@app.route('/warmup', methods=['GET'])
def warmup():
    try:
        chatbot.cleanup()
        result = chatbot.load_models()
        return jsonify({
            "status": result,
            "server_url": ngrok.get_tunnels()[0].public_url if ngrok.get_tunnels() else None
        })
    except Exception as e:
        logger.error(f"Error during warmup: {e}")
        return jsonify({
            "status": f"Error during warmup: {str(e)}",
            "server_url": None
        }), 500

@app.route('/send-to-flask', methods=['POST'])
def process_message():
    try:
        if not chatbot.models_loaded:
            chatbot.load_models()

        data = request.json
        prompt = data.get('prompt', '')
        image_data = data.get('image')

        if image_data:
            try:
                image_bytes = base64.b64decode(image_data.split(',')[1])
                image = Image.open(io.BytesIO(image_bytes))
                response = chatbot.chat(prompt, image)
            except Exception as e:
                logger.error(f"Image processing error: {e}")
                return jsonify({"error": f"Image processing error: {str(e)}"}), 400
        else:
            response = chatbot.chat(prompt)

        return jsonify({"response": response})
    except Exception as e:
        logger.error(f"Server error: {e}")
        return jsonify({"error": f"Server error: {str(e)}"}), 500

def initialize_ngrok():
    """Initialize ngrok with authentication if token is available"""
    try:
        ngrok_token = "2sTURZwzNsivA1BfxjA8q3qLgJ5_6msYqLKB8aD9wRsasqzfA"
        if ngrok_token:
            ngrok.set_auth_token(ngrok_token)
        return True
    except Exception as e:
        logger.error(f"Error initializing ngrok: {e}")
        return False

if __name__ == "__main__":
    port = 5000

    # Initialize ngrok
    if initialize_ngrok():
        try:
            public_url = ngrok.connect(port)
            print(f"\nðŸš€ Server is running!")
            print(f"ðŸ“¡ Ngrok tunnel: {public_url}")
            print(f"ðŸ”— Local URL: http://localhost:{port}")
        except Exception as e:
            logger.error(f"Error creating ngrok tunnel: {e}")

    # Run the Flask app
    app.run(host='0.0.0.0', port=port)

